[{"content":" 虽然以生成式AI作为研究方向，但是对与各种生成式模型实在是一知半解，缺乏深入的认识。网络上的教程和解读纷繁多样，即使看过不少，不过大都囫囵吞枣，早已成过眼云烟。 😮‍💨 最近正好发现22年出版的《Deep Generative Modeling》（作者：Jakub M. Tomczak）一书，是个系统学习深度生成模型的好机会。秉着好记性不如烂笔头的精神，打算通读一遍并将其中的内容翻译记录下来，作为加深记忆理解之用。限于英语水平，且有些内容融合了个人看法，所做翻译未必与原文完全一致。\n本书的序由机器学习领域大牛Max Welling而作，通读下来，感觉更像一篇随笔，其中没有对具体技术做深入阐述，只描述了对当前AI的一些看法，但其思考的问题和方式却值得学习借鉴，故将序言翻译整理如下：\n在过去十年中，深度学习的崛起引领了人工智能领域的巨大进展，彻底改变了众多子领域，例如计算机视觉、语音识别和自然语言处理等。此时此刻，更多的领域正在被颠覆重塑，包括机器人学、无线通信以及各种自然科学。\n其中大部分进展都起源于监督学习。在监督学习范式下，模型的训练数据带有标注，每个样本都有对应的标签。借助于标注数据，深度神经网络在图像目标识别、翻译等任务上已取得显著成绩。但是，数据的标注过程通常十分耗时且昂贵，甚至存在道德风险或完全无法实现。因此，研究者们已意识到，无监督（或自监督）学习才是引领日后进展的关键。\n无监督学习和自监督学习与人类的学习方式类似。举例来说，在儿童的成长历程中，学习所用的信息大都无任何标记。否则的话，难道曾经时时刻刻都有人在你耳边告诉你看到了什么，听到了什么？当然不是这样， 事实与之相反，我们必须在无监督的情况下学习世界的运行规律 ▶人类的学习并非完全无监督，应是存在一部分监督 ，而且是通过掌握信息（数据）中的结构或模式来学习。数据中存在大量的结构知识！假设我们通过组合像素的值获得一幅图像，其结果极有可能是毫无意义的噪声； 另一方面，所有可能的像素组合（图像空间）中，绝大部分实例与我们迄今为止看到的任何图像都不一样，这意味着存在很多的数据和结构 ▶此处存疑：上下文逻辑关系是怎样的？ ，因此对于儿童来说需要学习的东西很多。\n当然，儿童在学习的过程中不只是这个世界的旁观者，他们其实是在不断地与环境互动。在玩耍时他们会根据现实的反馈验证他们对物理、社会和心理等法则的认知。当现实与预测不同时，他们会感到惊讶，并可能更新内部的认知模型，以便下次做出更好的预测。所以，我们可以合理地假设， 与环境互动的过程是达到所谓人类智能的关键 ▶具身智能 。这与强化学习有着明显的相似之处，在强化学习中，智能体规划下一步的行动并根据环境的反馈更新决策或策略模型。 但是，对于机器人来说，很难通过与现实世界的互动实现假设的验证或数据的标注。因此，使用大量数据进行学习的实用方法是无监督学习。这一领域目前获得了大量的关注，且取得了惊人的进展。只需瞧瞧那些由模型轻而易举自动生成的全新人脸图像，我们就可以体验到这一领域已取得了不可思议的进步。\n无监督学习有多种形式。这本书（Deep Generative Modeling）关注其中的一种，即概率生成模型，其目标之一是估计输入数据的概率分布，可用于采样生成全新的数据实例（例如人脸图像）。另一目标是学习输入数据的抽象表示，亦称表示学习。高层次的表示会将输入数据自动解耦（disentangling）成我们所熟知的概念及其关系，例如图片中的猫和狗。虽然“解耦”有着明确直观的含义，但事实表明对他进行正确定义是相当棘手的。上世纪90年代，研究人员将大脑认知与统计层面相独立的隐变量关联起来。认为大脑的目标是将高度相关的细粒度表示（例如视觉像素）转为相对独立的隐变量表示（例如抽象概念），后者是对前者的压缩，更高效且冗余更少，从而使大脑在高效处理信息的同时耗费更少的能量。\n学习 和压缩 是两个关系紧密的概念。学习可视为对数据的有损压缩，因为学习不是简单地记住数据，而是要根据数据获得泛化能力。机器学习就是将数据集中关键的模式信息转化到模型参数中，并丢弃其他无关的信息。类似地，当我们观察一副图像时，所感兴趣的是其中的抽象概念，例如出现的物体以及联系，而不是直接的像素信息。在此基础上，我们可以对这些对象进行推理，联想出各种各样的可能性。所以， 智能就是从刺激我们感官的大量低层次信息中提取出关键信息，然后进行表示以开展后续的思想活动 ▶此定义言之有理但过于片面，最后的思想活动才是重点 。但我们日常生活中所熟知的事物并不是完全独立。因此，人们试图从不同的角度定义解耦，比如等变性或因果关系。\n在没有标签的情况下，训练模型最简单的方式是学习输入数据的概率生成（或密度）模型。在概率生成模型这一领域， 许多方法以最大化输入的对数概率或其下界作为优化目标 ▶VAE、GAN、FLOW、Diffusion都在此范式内，是否存在其他的范式 。除了VAE和GAN，该书还介绍了正则化流、自回归模型、能量模型以及当下最炙手可热的深度扩散模型。\n生成模型之外的很多模型也具备学习数据表示的能力，且能够有效地提升下游预测任务。针对表示学习，已出现了多种无需标注数据的训练任务，例如，针对时序数据，根据当前状态预测未来状态；针对图像，预测某一区域在另一区域的左侧还是右侧；针对视频，预测其是正向播放还是逆向播放；针对文本，根据上下文完形填空。此类无监督学习一般称为自监督学习，尽管我必须承认这个术语在不同人口中似乎有不同的用法。很多方法都可归类到上述无监督学习的范式中，包括一些生成式模型。例如，变分自编码器（VAE）将输入压缩为后验分布，然后根据压缩信息重建样本，即预测输入是什么；生成对抗网络（GAN）就是预测一个给定的样本是真实的还是虚假生成的；噪声对比估计（NCE）可看作是在隐空间预测输入片段在空间或时间上是否接近。\n很难说这个领域未来会发生什么？但显然通用人工智能（AGI）的实现将十分依赖于无监督学习。有趣的是，针对通用人工智能的实现方式，目前学界分为两大阵营： 一方主张“提升规模”，认为将当前的技术应用到更大的模型上，并使用更多的数据和算力进行训练，高级智能就会自动涌现 ▶智能是涌现出来的吗？为什么生物群落没有涌现出高级的智能，是不是涌现智能有必要的条件？ ，进而实现AGI； 另一方认为我们需要新的理论和想法，比如推理、因果或常识 ▶下一个突破点在哪？ 。\n此外，还有一些越发重要和紧迫的问题，那就是人类应该怎样与这些模型共处：如何理解模型内部发生的事情，或者直接放弃可解释性？当模型比我们更了解我们自己时，我们的生活会发生什么改变？那些遵从算法推荐的人是否比那些抵制的人更容易成功？当模型生成的虚假数据逼真到真伪难辨，我们还能相信什么？ 当虚假信息泛滥之时，民主是否还能继续发挥功能 ▶🐶 ？无论如何有一点非常肯定，那就是这个领域是当前最炙手可热的方向之一，而本书就是涉足这个领域的绝佳入门。 但每个人也应该充分意识到，掌握这项新技术同时需要承担新的社会责任。让我们谨慎的推进这一领域的发展 ▶AGI的未来征途上必然会出现更多更大的问题 。\nMax Welling\n2021.10.30\n","permalink":"https://ltecod.github.io/posts/2023/%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E4%B8%80/","summary":"虽然以生成式AI作为研究方向，但是对与各种生成式模型实在是一知半解，缺乏深入的认识。网络上的教程和解读纷繁多样，即使看过不少，不过大都囫囵吞","title":"[译]深度生成建模（一）：前言"},{"content":"","permalink":"https://ltecod.github.io/about/","summary":"","title":"About"},{"content":"","permalink":"https://ltecod.github.io/faq/","summary":"","title":"Faq"},{"content":"本站（ltecod.github.io）文章均为原创。\n本站采用 CC BY-NC-ND 4.0 协议，转载时请遵守以下条款：\n保留署名 深思豌豆 保留原文链接 不将本作品用于商业目的 如有任何内容不慎侵犯您的版权，请与我联系 jensus.yang@hotmail.com\n","permalink":"https://ltecod.github.io/copyright/","summary":"本站（ltecod.github.io）文章均为原创。 本站采用 CC BY-NC-ND 4.0 协议，转载时请遵守以下条款： 保留署名 深思豌豆 保留原文链接 不将本作品用于商","title":"版权说明"},{"content":"","permalink":"https://ltecod.github.io/tags/","summary":"","title":"标签"}]