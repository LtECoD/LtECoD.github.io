<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[译]深度生成建模（一）：前言 | 豌豆道场</title><meta name=keywords content="深度生成模型"><meta name=description content="虽然以生成式AI作为研究方向，但是对与各种生成式模型实在是一知半解，缺乏深入的认识。网络上的教程和解读纷繁多样，即使看过不少，不过大都囫囵吞"><meta name=author content="神思豌豆"><link rel=canonical href=https://ltecod.github.io/posts/2023/%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E4%B8%80/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0d8b3a4cd83c2402d03a266ab444bb4983ea8e0806c8a55697cc0d11c174f86e.css integrity="sha256-DYs6TNg8JALQOiZqtES7SYPqjggGyKVWl8wNEcF0+G4=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://ltecod.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://ltecod.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://ltecod.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://ltecod.github.io/apple-touch-icon.png><link rel=mask-icon href=https://ltecod.github.io/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css integrity=sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js integrity=sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-3H93LFDVRR"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-3H93LFDVRR")</script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/lxgw-wenkai-screen-webfont/1.7.0/style.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;700&display=swap"><script async src="https://www.googletagmanager.com/gtag/js?id=G-3H93LFDVRR"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-3H93LFDVRR",{anonymize_ip:!1})}</script><meta property="og:title" content="[译]深度生成建模（一）：前言"><meta property="og:description" content="虽然以生成式AI作为研究方向，但是对与各种生成式模型实在是一知半解，缺乏深入的认识。网络上的教程和解读纷繁多样，即使看过不少，不过大都囫囵吞"><meta property="og:type" content="article"><meta property="og:url" content="https://ltecod.github.io/posts/2023/%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E4%B8%80/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-10-28T12:03:30+08:00"><meta property="article:modified_time" content="2023-10-28T12:03:30+08:00"><meta property="og:site_name" content="豌豆道场"><meta name=twitter:card content="summary"><meta name=twitter:title content="[译]深度生成建模（一）：前言"><meta name=twitter:description content="虽然以生成式AI作为研究方向，但是对与各种生成式模型实在是一知半解，缺乏深入的认识。网络上的教程和解读纷繁多样，即使看过不少，不过大都囫囵吞"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"文章","item":"https://ltecod.github.io/posts/"},{"@type":"ListItem","position":3,"name":"[译]深度生成建模（一）：前言","item":"https://ltecod.github.io/posts/2023/%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E4%B8%80/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[译]深度生成建模（一）：前言","name":"[译]深度生成建模（一）：前言","description":"虽然以生成式AI作为研究方向，但是对与各种生成式模型实在是一知半解，缺乏深入的认识。网络上的教程和解读纷繁多样，即使看过不少，不过大都囫囵吞","keywords":["深度生成模型"],"articleBody":" 虽然以生成式AI作为研究方向，但是对与各种生成式模型实在是一知半解，缺乏深入的认识。网络上的教程和解读纷繁多样，即使看过不少，不过大都囫囵吞枣，早已成过眼云烟。 😮‍💨 最近正好发现22年出版的《Deep Generative Modeling》（作者：Jakub M. Tomczak）一书，是个系统学习深度生成模型的好机会。秉着好记性不如烂笔头的精神，打算通读一遍并将其中的内容翻译记录下来，作为加深记忆理解之用。限于英语水平，且有些内容融合了个人看法，所做翻译未必与原文完全一致。\n本书的序由机器学习领域大牛Max Welling而作，通读下来，感觉更像一篇随笔，其中没有对具体技术做深入阐述，只描述了对当前AI的一些看法，但其思考的问题和方式却值得学习借鉴，故将序言翻译整理如下：\n在过去十年中，深度学习的崛起引领了人工智能领域的巨大进展，彻底改变了众多子领域，例如计算机视觉、语音识别和自然语言处理等。此时此刻，更多的领域正在被颠覆重塑，包括机器人学、无线通信以及各种自然科学。\n其中大部分进展都起源于监督学习。在监督学习范式下，模型的训练数据带有标注，每个样本都有对应的标签。借助于标注数据，深度神经网络在图像目标识别、翻译等任务上已取得显著成绩。但是，数据的标注过程通常十分耗时且昂贵，甚至存在道德风险或完全无法实现。因此，研究者们已意识到，无监督（或自监督）学习才是引领日后进展的关键。\n无监督学习和自监督学习与人类的学习方式类似。举例来说，在儿童的成长历程中，学习所用的信息大都无任何标记。否则的话，难道曾经时时刻刻都有人在你耳边告诉你看到了什么，听到了什么？当然不是这样， 事实与之相反，我们必须在无监督的情况下学习世界的运行规律 ▶人类的学习并非完全无监督，应是存在一部分监督 ，而且是通过掌握信息（数据）中的结构或模式来学习。数据中存在大量的结构知识！假设我们通过组合像素的值获得一幅图像，其结果极有可能是毫无意义的噪声； 另一方面，所有可能的像素组合（图像空间）中，绝大部分实例与我们迄今为止看到的任何图像都不一样，这意味着存在很多的数据和结构 ▶此处存疑：上下文逻辑关系是怎样的？ ，因此对于儿童来说需要学习的东西很多。\n当然，儿童在学习的过程中不只是这个世界的旁观者，他们其实是在不断地与环境互动。在玩耍时他们会根据现实的反馈验证他们对物理、社会和心理等法则的认知。当现实与预测不同时，他们会感到惊讶，并可能更新内部的认知模型，以便下次做出更好的预测。所以，我们可以合理地假设， 与环境互动的过程是达到所谓人类智能的关键 ▶具身智能 。这与强化学习有着明显的相似之处，在强化学习中，智能体规划下一步的行动并根据环境的反馈更新决策或策略模型。 但是，对于机器人来说，很难通过与现实世界的互动实现假设的验证或数据的标注。因此，使用大量数据进行学习的实用方法是无监督学习。这一领域目前获得了大量的关注，且取得了惊人的进展。只需瞧瞧那些由模型轻而易举自动生成的全新人脸图像，我们就可以体验到这一领域已取得了不可思议的进步。\n无监督学习有多种形式。这本书（Deep Generative Modeling）关注其中的一种，即概率生成模型，其目标之一是估计输入数据的概率分布，可用于采样生成全新的数据实例（例如人脸图像）。另一目标是学习输入数据的抽象表示，亦称表示学习。高层次的表示会将输入数据自动解耦（disentangling）成我们所熟知的概念及其关系，例如图片中的猫和狗。虽然“解耦”有着明确直观的含义，但事实表明对他进行正确定义是相当棘手的。上世纪90年代，研究人员将大脑认知与统计层面相独立的隐变量关联起来。认为大脑的目标是将高度相关的细粒度表示（例如视觉像素）转为相对独立的隐变量表示（例如抽象概念），后者是对前者的压缩，更高效且冗余更少，从而使大脑在高效处理信息的同时耗费更少的能量。\n学习 和压缩 是两个关系紧密的概念。学习可视为对数据的有损压缩，因为学习不是简单地记住数据，而是要根据数据获得泛化能力。机器学习就是将数据集中关键的模式信息转化到模型参数中，并丢弃其他无关的信息。类似地，当我们观察一副图像时，所感兴趣的是其中的抽象概念，例如出现的物体以及联系，而不是直接的像素信息。在此基础上，我们可以对这些对象进行推理，联想出各种各样的可能性。所以， 智能就是从刺激我们感官的大量低层次信息中提取出关键信息，然后进行表示以开展后续的思想活动 ▶此定义言之有理但过于片面，最后的思想活动才是重点 。但我们日常生活中所熟知的事物并不是完全独立。因此，人们试图从不同的角度定义解耦，比如等变性或因果关系。\n在没有标签的情况下，训练模型最简单的方式是学习输入数据的概率生成（或密度）模型。在概率生成模型这一领域， 许多方法以最大化输入的对数概率或其下界作为优化目标 ▶VAE、GAN、FLOW、Diffusion都在此范式内，是否存在其他的范式 。除了VAE和GAN，该书还介绍了正则化流、自回归模型、能量模型以及当下最炙手可热的深度扩散模型。\n生成模型之外的很多模型也具备学习数据表示的能力，且能够有效地提升下游预测任务。针对表示学习，已出现了多种无需标注数据的训练任务，例如，针对时序数据，根据当前状态预测未来状态；针对图像，预测某一区域在另一区域的左侧还是右侧；针对视频，预测其是正向播放还是逆向播放；针对文本，根据上下文完形填空。此类无监督学习一般称为自监督学习，尽管我必须承认这个术语在不同人口中似乎有不同的用法。很多方法都可归类到上述无监督学习的范式中，包括一些生成式模型。例如，变分自编码器（VAE）将输入压缩为后验分布，然后根据压缩信息重建样本，即预测输入是什么；生成对抗网络（GAN）就是预测一个给定的样本是真实的还是虚假生成的；噪声对比估计（NCE）可看作是在隐空间预测输入片段在空间或时间上是否接近。\n很难说这个领域未来会发生什么？但显然通用人工智能（AGI）的实现将十分依赖于无监督学习。有趣的是，针对通用人工智能的实现方式，目前学界分为两大阵营： 一方主张“提升规模”，认为将当前的技术应用到更大的模型上，并使用更多的数据和算力进行训练，高级智能就会自动涌现 ▶智能是涌现出来的吗？为什么生物群落没有涌现出高级的智能，是不是涌现智能有必要的条件？ ，进而实现AGI； 另一方认为我们需要新的理论和想法，比如推理、因果或常识 ▶下一个突破点在哪？ 。\n此外，还有一些越发重要和紧迫的问题，那就是人类应该怎样与这些模型共处：如何理解模型内部发生的事情，或者直接放弃可解释性？当模型比我们更了解我们自己时，我们的生活会发生什么改变？那些遵从算法推荐的人是否比那些抵制的人更容易成功？当模型生成的虚假数据逼真到真伪难辨，我们还能相信什么？ 当虚假信息泛滥之时，民主是否还能继续发挥功能 ▶🐶 ？无论如何有一点非常肯定，那就是这个领域是当前最炙手可热的方向之一，而本书就是涉足这个领域的绝佳入门。 但每个人也应该充分意识到，掌握这项新技术同时需要承担新的社会责任。让我们谨慎的推进这一领域的发展 ▶AGI的未来征途上必然会出现更多更大的问题 。\nMax Welling\n2021.10.30\n","wordCount":"2880","inLanguage":"zh","datePublished":"2023-10-28T12:03:30+08:00","dateModified":"2023-10-28T12:03:30+08:00","author":{"@type":"Person","name":"神思豌豆"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ltecod.github.io/posts/2023/%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E4%B8%80/"},"publisher":{"@type":"Organization","name":"豌豆道场","logo":{"@type":"ImageObject","url":"https://ltecod.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://ltecod.github.io accesskey=h title="豌豆道场 (Alt + H)">豌豆道场</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://ltecod.github.io/archives/ title=归档><span>归档</span></a></li><li><a href=https://ltecod.github.io/search/ title=搜索><span>搜索</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://ltecod.github.io>主页</a>&nbsp;»&nbsp;<a href=https://ltecod.github.io/posts/>文章</a></div><h1 class=post-title>[译]深度生成建模（一）：前言</h1><div class=post-meta><span title='2023-10-28 12:03:30 +0800 CST'>2023-10-28</span>&nbsp;·&nbsp;神思豌豆</div></header><div class=post-content><figure class=align-center><img loading=lazy src=/2023_image/1.1.dgm.png#center width=50%></figure><div class=notice><div class=notice-head><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 9v5m0 5c-4.97056.0-9-4.0294-9-9 0-4.97056 4.02944-9 9-9 4.9706.0 9 4.02944 9 9 0 4.9706-4.0294 9-9 9zM10.0498 6V6.1L9.9502 6.1002V6H10.0498z" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg><p></p></div><div class=notice-body><p><em>虽然以生成式AI作为研究方向，但是对与各种生成式模型实在是一知半解，缺乏深入的认识。网络上的教程和解读纷繁多样，即使看过不少，不过大都囫囵吞枣，早已成过眼云烟。</em>
😮‍💨
<em>最近正好发现22年出版的《Deep Generative Modeling》（作者：Jakub M. Tomczak）一书，是个系统学习深度生成模型的好机会。秉着好记性不如烂笔头的精神，打算通读一遍并将其中的内容翻译记录下来，作为加深记忆理解之用。限于英语水平，且有些内容融合了个人看法，所做翻译未必与原文完全一致。</em></p></div></div><blockquote><p><strong>本书的序由机器学习领域大牛Max Welling而作，通读下来，感觉更像一篇随笔，其中没有对具体技术做深入阐述，只描述了对当前AI的一些看法，但其思考的问题和方式却值得学习借鉴，故将序言翻译整理如下：</strong></p></blockquote><p>在过去十年中，深度学习的崛起引领了人工智能领域的巨大进展，彻底改变了众多子领域，例如计算机视觉、语音识别和自然语言处理等。此时此刻，更多的领域正在被颠覆重塑，包括机器人学、无线通信以及各种自然科学。</p><p>其中大部分进展都起源于监督学习。在监督学习范式下，模型的训练数据带有标注，每个样本都有对应的标签。借助于标注数据，深度神经网络在图像目标识别、翻译等任务上已取得显著成绩。但是，数据的标注过程通常十分耗时且昂贵，甚至存在道德风险或完全无法实现。因此，研究者们已意识到，无监督（或自监督）学习才是引领日后进展的关键。</p><p>无监督学习和自监督学习与人类的学习方式类似。举例来说，在儿童的成长历程中，学习所用的信息大都无任何标记。否则的话，难道曾经时时刻刻都有人在你耳边告诉你看到了什么，听到了什么？当然不是这样，<html><b>事实与之相反，我们必须在无监督的情况下学习世界的运行规律</b>
<sub>▶人类的学习并非完全无监督，应是存在一部分监督</sub></html>，而且是通过掌握信息（数据）中的结构或模式来学习。数据中存在大量的结构知识！假设我们通过组合像素的值获得一幅图像，其结果极有可能是毫无意义的噪声；<html><b>另一方面，所有可能的像素组合（图像空间）中，绝大部分实例与我们迄今为止看到的任何图像都不一样，这意味着存在很多的数据和结构</b>
<sub>▶此处存疑：上下文逻辑关系是怎样的？</sub></html>，因此对于儿童来说需要学习的东西很多。</p><p>当然，儿童在学习的过程中不只是这个世界的旁观者，他们其实是在不断地与环境互动。在玩耍时他们会根据现实的反馈验证他们对物理、社会和心理等法则的认知。当现实与预测不同时，他们会感到惊讶，并可能更新内部的认知模型，以便下次做出更好的预测。所以，我们可以合理地假设，<html><b>与环境互动的过程是达到所谓人类智能的关键</b>
<sub>▶具身智能</sub></html>。这与强化学习有着明显的相似之处，在强化学习中，智能体规划下一步的行动并根据环境的反馈更新决策或策略模型。
但是，对于机器人来说，很难通过与现实世界的互动实现假设的验证或数据的标注。因此，使用大量数据进行学习的实用方法是无监督学习。这一领域目前获得了大量的关注，且取得了惊人的进展。只需瞧瞧那些由模型轻而易举自动生成的全新人脸图像，我们就可以体验到这一领域已取得了不可思议的进步。</p><p>无监督学习有多种形式。这本书（Deep Generative Modeling）关注其中的一种，即概率生成模型，其目标之一是估计输入数据的概率分布，可用于采样生成全新的数据实例（例如人脸图像）。另一目标是学习输入数据的抽象表示，亦称表示学习。高层次的表示会将输入数据自动解耦（disentangling）成我们所熟知的概念及其关系，例如图片中的猫和狗。虽然“解耦”有着明确直观的含义，但事实表明对他进行正确定义是相当棘手的。上世纪90年代，研究人员将大脑认知与统计层面相独立的隐变量关联起来。认为大脑的目标是将高度相关的细粒度表示（例如视觉像素）转为相对独立的隐变量表示（例如抽象概念），后者是对前者的压缩，更高效且冗余更少，从而使大脑在高效处理信息的同时耗费更少的能量。</p><p><mark><b>学习</b></mark>
和<mark><b>压缩</b></mark>
是两个关系紧密的概念。学习可视为对数据的有损压缩，因为学习不是简单地记住数据，而是要根据数据获得泛化能力。机器学习就是将数据集中关键的模式信息转化到模型参数中，并丢弃其他无关的信息。类似地，当我们观察一副图像时，所感兴趣的是其中的抽象概念，例如出现的物体以及联系，而不是直接的像素信息。在此基础上，我们可以对这些对象进行推理，联想出各种各样的可能性。所以，<html><b>智能就是从刺激我们感官的大量低层次信息中提取出关键信息，然后进行表示以开展后续的思想活动</b>
<sub>▶此定义言之有理但过于片面，最后的思想活动才是重点</sub></html>。但我们日常生活中所熟知的事物并不是完全独立。因此，人们试图从不同的角度定义解耦，比如等变性或因果关系。</p><p>在没有标签的情况下，训练模型最简单的方式是学习输入数据的概率生成（或密度）模型。在概率生成模型这一领域，<html><b>许多方法以最大化输入的对数概率或其下界作为优化目标</b>
<sub>▶VAE、GAN、FLOW、Diffusion都在此范式内，是否存在其他的范式</sub></html>。除了VAE和GAN，该书还介绍了正则化流、自回归模型、能量模型以及当下最炙手可热的深度扩散模型。</p><p>生成模型之外的很多模型也具备学习数据表示的能力，且能够有效地提升下游预测任务。针对表示学习，已出现了多种无需标注数据的训练任务，例如，针对时序数据，根据当前状态预测未来状态；针对图像，预测某一区域在另一区域的左侧还是右侧；针对视频，预测其是正向播放还是逆向播放；针对文本，根据上下文完形填空。此类无监督学习一般称为自监督学习，尽管我必须承认这个术语在不同人口中似乎有不同的用法。很多方法都可归类到上述无监督学习的范式中，包括一些生成式模型。例如，变分自编码器（VAE）将输入压缩为后验分布，然后根据压缩信息重建样本，即预测输入是什么；生成对抗网络（GAN）就是预测一个给定的样本是真实的还是虚假生成的；噪声对比估计（NCE）可看作是在隐空间预测输入片段在空间或时间上是否接近。</p><p>很难说这个领域未来会发生什么？但显然通用人工智能（AGI）的实现将十分依赖于无监督学习。有趣的是，针对通用人工智能的实现方式，目前学界分为两大阵营：<html><b>一方主张“提升规模”，认为将当前的技术应用到更大的模型上，并使用更多的数据和算力进行训练，高级智能就会自动涌现</b>
<sub>▶智能是涌现出来的吗？为什么生物群落没有涌现出高级的智能，是不是涌现智能有必要的条件？</sub></html>，进而实现AGI；<html><b>另一方认为我们需要新的理论和想法，比如推理、因果或常识</b>
<sub>▶下一个突破点在哪？</sub></html>。</p><p>此外，还有一些越发重要和紧迫的问题，那就是人类应该怎样与这些模型共处：如何理解模型内部发生的事情，或者直接放弃可解释性？当模型比我们更了解我们自己时，我们的生活会发生什么改变？那些遵从算法推荐的人是否比那些抵制的人更容易成功？当模型生成的虚假数据逼真到真伪难辨，我们还能相信什么？<html><b>当虚假信息泛滥之时，民主是否还能继续发挥功能</b>
<sub>▶🐶</sub></html>？无论如何有一点非常肯定，那就是这个领域是当前最炙手可热的方向之一，而本书就是涉足这个领域的绝佳入门。<html><b>但每个人也应该充分意识到，掌握这项新技术同时需要承担新的社会责任。让我们谨慎的推进这一领域的发展</b>
<sub>▶AGI的未来征途上必然会出现更多更大的问题</sub></html>。</p><p><p style=text-align:right>Max Welling</p><p style=text-align:right>2021.10.30</p></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://ltecod.github.io/tags/%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/>深度生成模型</a></li></ul></footer><div class=giscus><script>function getGiscusTheme(){const e=localStorage.getItem("pref-theme"),t=e==null||e=="light"?"light":"noborder_dark";return t}function setGiscusTheme(){function e(e){const t=document.querySelector("iframe.giscus-frame");if(!t)return;t.contentWindow.postMessage({giscus:e},"https://giscus.app")}e({setConfig:{theme:getGiscusTheme()=="light"?"noborder_dark":"light"}})}const attrs={src:"https://giscus.app/client.js","data-repo":"LtECoD/LtECoD.github.io","data-repo-id":"R_kgDOKiPsWA","data-category":"Announcements","data-category-id":"DIC_kwDOKiPsWM4Caly0","data-mapping":"pathname","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":getGiscusTheme(),"data-lang":"zh-CN",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(attrs).forEach(([e,t])=>giscusScript.setAttribute(e,t)),document.body.appendChild(giscusScript);const toggle=document.querySelector("#theme-toggle");toggle&&toggle.addEventListener("click",setGiscusTheme)</script></div></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><footer class=footer><span>&copy; 2023 <a href=https://ltecod.github.io>豌豆道场</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span>
<span>·&nbsp;<a href=/copyright/>版权说明</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>